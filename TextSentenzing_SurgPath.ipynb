{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample scripts\n",
    "import pandas as pd\n",
    "import os\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import ast\n",
    "data_dir = os.getcwd()\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#print(data_dir)\n",
    "cohort_df = pd.read_csv(data_dir + '/data/chrtDf.csv')\n",
    "Pnotes_df = pd.read_pickle('Pnotesfile.pickle')\n",
    "RadPath_df = pd.read_pickle('RadPathfile.pickle')\n",
    "SurgPath_df = pd.read_pickle('SurgPathfile.pickle')\n",
    "cohort_df.rename(columns =  {'tnm_mixed_stage_desc' : 'levels'},inplace= True)\n",
    "cohort_df = cohort_df.loc[cohort_df['pat_id']!='NOPE']\n",
    "pat_count = cohort_df.groupby('pat_id').size().sort_values(ascending=False)\n",
    "# single record seperation\n",
    "patsWithOne = pat_count.loc[pat_count == 1]\n",
    "chrtDfOne = cohort_df.loc[cohort_df['pat_id'].isin(list(patsWithOne.index))].reset_index()\n",
    "#Remove records with no labels or missing labels\n",
    "\n",
    "chrtDf = chrtDfOne[chrtDfOne.levels != 'Unknown']\n",
    "chrtDf = chrtDfOne[chrtDfOne.levels != 'Missing']\n",
    "chrtDf = chrtDfOne[chrtDfOne.levels != 'Not Applicable']\n",
    "chrtDf = chrtDfOne[chrtDfOne['gender'].notnull()]\n",
    "chrtDf = chrtDfOne[chrtDfOne['bmi'].notnull()]\n",
    "\n",
    "#stripping the subclass of cancer staging data; eg. 1A to 1\n",
    "chrtDf['levels'] = chrtDf['levels'].apply(lambda x: x[0])\n",
    "#print(text)\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SurgPath_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rows that have no text or '[ ]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3551, 27)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SurgPath_df.dropna(subset=['note_text'], inplace =True)\n",
    "SurgPath_df = SurgPath_df[SurgPath_df['nlp_text_qu_ner']!='[]']\n",
    "SurgPath_df.reset_index(inplace = True)\n",
    "SurgPath_df.shape\n",
    "SurgPath_df.reset_index(drop = True)\n",
    "SurgPath_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NLTK sentence tokenizer to find out average number of sentences in each note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.023373697549985"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SurgSenLen =[]\n",
    "for i in SurgPath_df.index:\n",
    "    text =SurgPath_df.loc[i]['note_text']\n",
    "    doc = sent_tokenize(text)\n",
    "    SurgSenLen.append(len(doc))\n",
    "# for i, token in enumerate(doc):\n",
    "#     print('-->Sentence %d: %s' % (i, token))\n",
    "np.mean(np.array(SurgSenLen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add new columns to RadPath data frame\n",
    "### Columns: number of sentences in the text, longest and shortest and average number of words in each sentence in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "SurgPath_df =SurgPath_df.assign(**{'Number of sentences': np.nan, 'Longest sentence length (words)': np.nan, \\\n",
    "                                 'Shortest sentence length(words)': np.nan, \\\n",
    "                    'average sentence length(words)':np.nan})\n",
    "# RadPath_df.assign(Number_of_sentences = np.nan, Longest_sentence_length = np.nan, Shortest_sentence_length = np.nan, \\\n",
    "#                    Average_sentence_length= np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add all the sentence lengths in the radpath text notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Number_sentence(text):\n",
    "    return len(sent_tokenize(text))\n",
    "\n",
    "SurgPath_df['Number of sentences'] = SurgPath_df['note_text'].apply(Number_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add statistics for each note text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from statistics import mean\n",
    "def word_count(doc):\n",
    "    lenWordHolder =[]\n",
    "    for i, token in enumerate(doc):\n",
    "        \n",
    "        lenWordHolder.append(len(word_tokenize(token)))\n",
    "    return lenWordHolder\n",
    "    \n",
    "def max_length(text):\n",
    "    doc = sent_tokenize(text)\n",
    "    len_word = word_count(doc)\n",
    "    #print(max(len_word))\n",
    "    return max(len_word)\n",
    "\n",
    "def min_length(text):\n",
    "    doc = sent_tokenize(text)\n",
    "    len_word = word_count(doc)\n",
    "#     print(min(len_word))\n",
    "#     print('\\n')\n",
    "#     print(doc)\n",
    "    return min(len_word)\n",
    "    \n",
    "def avg_length(text):\n",
    "    doc = sent_tokenize(text)\n",
    "    len_word = word_count(doc)\n",
    "    return mean(len_word)\n",
    "        \n",
    "SurgPath_df['Longest sentence length (words)'] = SurgPath_df['note_text'].apply(max_length)\n",
    "SurgPath_df['Shortest sentence length(words)'] = SurgPath_df['note_text'].apply(min_length)\n",
    "SurgPath_df['average sentence length(words)'] = SurgPath_df['note_text'].apply(avg_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3503,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SurgPath_df['note_text'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SurgPath_df_analysis = SurgPath_df[['pat_id','result_time','note_text', 'Number of sentences','Longest sentence length (words)',\\\n",
    "                               'Shortest sentence length(words)', 'average sentence length(words)', 'nlp_text_qu_ner' ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17     0.046466\n",
       "19     0.043368\n",
       "15     0.043086\n",
       "21     0.041960\n",
       "20     0.041397\n",
       "         ...   \n",
       "97     0.000282\n",
       "84     0.000282\n",
       "107    0.000282\n",
       "119    0.000282\n",
       "124    0.000282\n",
       "Name: Number of sentences, Length: 103, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RadPath_analysis.hist(column= 'Number of sentences',bins= 50, grid=True, figsize=(12,8), color='red', zorder=2, rwidth=0.9 )\n",
    "#RadPath_analysis['Number of sentences'].hist(bins =50)\n",
    "SurgPath_df_analysis['Number of sentences'].value_counts(normalize=True, ascending= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SurgPath reports are more unique and have content as compared to RadPath reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "sns.distplot(SurgPath_df_analysis['Number of sentences'], hist=True, kde=False, \n",
    "             color = 'blue',\n",
    "             hist_kws={'edgecolor':'black'});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the note_texts to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "SurgPath_df_analysis['note_text'] = SurgPath_df_analysis['note_text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check with QUICKUMLS output for any useful concept from these notes containing only one sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "SurgPath_df_analysis =SurgPath_df_analysis.assign(**{'T023 output': np.nan, 'T191 output': np.nan})\n",
    "SurgPath_df_analysis.reset_index(inplace= True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To check if concept extraction is useful from semstring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_T023_total = []\n",
    "corpus_T191_total = []\n",
    "for i in range(len(SurgPath_df_analysis.index)):\n",
    "    \n",
    "    text = SurgPath_df_analysis.iloc[i].nlp_text_qu_ner\n",
    "    d = ast.literal_eval(text)\n",
    "    Testframe = pd.DataFrame.from_dict(d)\n",
    "    corpus_T023 = Testframe[Testframe['semtypestring'] == 'T023']['ngram'].tolist()\n",
    "    corpus_T191 = Testframe[Testframe['semtypestring'] == 'T191']['ngram'].tolist()\n",
    "    corpus_T023_total.append(corpus_T023)\n",
    "    corpus_T191_total.append(corpus_T191)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only few are empty\n",
    "### 2.59% for T023\n",
    "### 2.87 % for T191\n",
    "## It appears that Surgpath reports are carrying more information than RadPath report on average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.590819487468321\n",
      "2.8724303013235697\n"
     ]
    }
   ],
   "source": [
    "corpus_T023_total\n",
    "T023 = [e for e in corpus_T023_total if e]\n",
    "print(100*(1- (len(T023)/len(corpus_T023_total))))\n",
    "\n",
    "T191 = [e for e in corpus_T191_total if e]\n",
    "print(100*(1- (len(T191)/len(corpus_T191_total))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RadPath_analysis.groupby([['pat_id','result_time','note_text', 'Number of sentences','Longest sentence length',\\\n",
    "#                                'Shortest sentence length', 'average sentence length']])\n",
    "SurgPath_df_analysis.groupby(['pat_id','result_time']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec model from the words in the sentences\n",
    "### word vectorization with TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer_TFIDF = TfidfVectorizer()\n",
    "vectorizer_CV = CountVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cohort =[]\n",
    "for i in range(len(SurgPath_df_analysis.index)):\n",
    "    text = SurgPath_df_analysis.loc[i,'note_text']\n",
    "    text_cohort.append(text)\n",
    "#     word_input = word_tokenize(text)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     text_CV= vectorizer_CV.fit_transform(word_input)\n",
    "#     text_TFIDF = vectorizer_TFIDF.fit_transform(word_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use TFIDF method to find out the words that has propotional representation in he corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_CV= vectorizer_CV.fit_transform(text_cohort)\n",
    "text_TFIDF = vectorizer_TFIDF.fit_transform(text_cohort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_CV.get_feature_names()[np.sum(text_CV.toarray(),axis =0).argmax()]\n",
    "vectorizer_TFIDF.get_feature_names()[np.sum(text_TFIDF.toarray(),axis =0).argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(text_TFIDF.toarray(),axis =0)\n",
    "#np.sum(text_CV.toarray(),axis =0)\n",
    "# vectorizer_TFIDF.get_feature_names()[np.sum(text_TFIDF.toarray(),axis =0).argmin()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement word2vec on th text cohort from the single sentence sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cohort\n",
    "all_words = [nltk.word_tokenize(sent) for sent in text_cohort]\n",
    "# Removing Stop Words\n",
    "from nltk.corpus import stopwords\n",
    "for i in range(len(all_words)):\n",
    "    all_words[i] = [w for w in all_words[i] if w not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec_model = Word2Vec(all_words, min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = word2vec_model.wv.vocab\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = word2vec_model.wv['test']\n",
    "v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dbasu/.local/share/virtualenvs/ucd-ri-csnlp-AG0bO8yr/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "f_name = 'wor2vec_model.txt'\n",
    "word2vec_model.wv.save_word2vec_format(f_name, binary = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement doc2vec on the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the dependencies\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 100\n",
    "vec_size = 20\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "  \n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(fname, tokens_only=False):\n",
    "    with smart_open.smart_open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if tokens_only:\n",
    "                yield gensim.utils.simple_preprocess(line)\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = list(read_corpus(lee_train_file))\n",
    "test_corpus = list(read_corpus(lee_test_file, tokens_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(train_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.34 s, sys: 135 ms, total: 4.48 s\n",
      "Wall time: 1.91 s\n"
     ]
    }
   ],
   "source": [
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SpaCy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RadSenLen =[]\n",
    "for i in RadPath_df.index:\n",
    "    text =RadPath_df.loc[i]['note_text']\n",
    "    doc = nlp(text)\n",
    "    RadSenLen.append(len(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return the indices of rows with number of sentences equal to 1\n",
    "res_list = list(filter(lambda x: RadSenLen[x] == 3, range(len(RadSenLen)))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "i = random.choice(res_list)\n",
    "print(i)\n",
    "text =RadPath_df.iloc[i]['note_text']\n",
    "doc = nlp(text)\n",
    "for i, token in enumerate(doc):\n",
    "    print('-->Sentence %d: %s' % (i, token))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
